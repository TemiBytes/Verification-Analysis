{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfae1126",
   "metadata": {},
   "source": [
    "VERIFICATIONS DATA CLEANING\n",
    "\n",
    "<p>The purpose of this analysis is to clean this dataset before loading into MYSQL for analysis</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e5ebbe",
   "metadata": {},
   "source": [
    "1) Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcb682d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d5da61",
   "metadata": {},
   "source": [
    "2. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58861971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load verifications sheet into pandas dataframe, set low_memory=False to avoid datatype guessing errors for large files\n",
    "df = pd.read_csv(r'C:\\Users\\pc\\OneDrive\\Documents\\Projects\\Verification project\\verification_data.csv', low_memory=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a2ffba",
   "metadata": {},
   "source": [
    "3. Preview the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf30fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the first five rows of data to understand the structure\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3306b233",
   "metadata": {},
   "source": [
    "4. Check Dataset information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068ad74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the dataframe info and data types\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e9c272",
   "metadata": {},
   "source": [
    "5. Convert columns to their right data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34df641c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#---1 convert date_time and review_date to datetime columns\n",
    "df['datetime'] = pd.to_datetime(df['datetime']).dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "df['review_date'] = pd.to_datetime(df['review_date']).dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "\n",
    "#---2 convert review_status, direct_feedback and duplicate_check to Int64\n",
    "df['review_status'] = df['review_status'].astype('Int64')\n",
    "df['direct_feedback'] = df['direct_feedback'].astype('Int64')\n",
    "df['duplicate_check'] = df['duplicate_check'].astype('Int64')\n",
    "df['address_exists'] = df['address_exists'].astype('Int64')\n",
    "\n",
    "\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d18bee",
   "metadata": {},
   "source": [
    "6. Check Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6595ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#shows how many missing values exist in each columns\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3a9a1c",
   "metadata": {},
   "source": [
    "7. Standardize Values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d44b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert all options in status to proper format for consistency\n",
    "df['status'] = df['status'].str.title() #completed -> Completed\n",
    "df['status'].unique() #checks for distinct values in the status column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f2c089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert all id_types to a standardized format \n",
    "df['id_type'] = df['id_type'].str.upper()\n",
    "df['id_type'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de0ecd9",
   "metadata": {},
   "source": [
    "8. Remove Whitespaces "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abcbb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# strip leading and white spaces from all text columns\n",
    "\n",
    "df = df.apply(lambda x: x.str.strip() if x.dtype == 'object' else x) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc8a007",
   "metadata": {},
   "source": [
    "9. Handle Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3996ff2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle missing data based on the nature of each column\n",
    "df.fillna({\n",
    "    'selfie_url': 'No selfie',\n",
    "    'id_url': 'No id url',\n",
    "    'mode': 'none',\n",
    "    'id_type' : 'none',\n",
    "    'aml_reference': 'none',\n",
    "    'review_comment': 'none',\n",
    "    'id_number': 'none',\n",
    "    'country': 'unknown',\n",
    "    'r_action': 'none',\n",
    "    'back_url': 'none',\n",
    "    'address': 'unknown',\n",
    "    'review_status': -1,\n",
    "    'ucode': 'none',\n",
    "    'reviewed_by': 'none',\n",
    "    'reviewer': 'none',\n",
    "    'failed_id_capture': 'not available',\n",
    "    'email': 'none',\n",
    "    'reason_for_human_review_id': 'none',\n",
    "    'reason_for_human_review_selfie': 'none',\n",
    "    'duplicate_check': 0,\n",
    "    'address_exists':0,\n",
    "    'direct_feedback': 0,\n",
    "    'flow_id': 'none',\n",
    "    'customer_id': 'none'\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b9e5ba",
   "metadata": {},
   "source": [
    "Inspect the Country column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a75a636",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inspect the country column for dirty entries\n",
    "print(' the number of distinct entries in the country column is :',len(df.country.unique()))\n",
    "print(df.country.unique()) # result shows this list of all distinct countries in the column. \n",
    "# From this inspection it shows how messy the column is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b044241a",
   "metadata": {},
   "source": [
    "11. Fuzzy Cleaning With Python - Import Geolocation + Fuzzy Matching Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33cb2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycountry\n",
    "from fuzzywuzzy import process\n",
    "from geopy.geocoders import Nominatim\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "#imported tools needed to clean country names automatically"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf07306f",
   "metadata": {},
   "source": [
    "12. Setup Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f80965",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up geopy and country_list\n",
    "\n",
    "geolocator = Nominatim(user_agent=\"geo_cleaner\")\n",
    "valid_countries = [country.name for country in pycountry.countries] #built in list of valid countries from pycountry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a94bab4",
   "metadata": {},
   "source": [
    "13. Define Country Cleaning Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4102f3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define fuzzy cleaning function\n",
    "\n",
    "def clean_country(value):\n",
    "    value  = str(value).strip()\n",
    "\n",
    "    if pd.isna(value) or '@' in str(value) or str(value).isdigit() or len(str(value).strip()) < 3:\n",
    "        return 'unknown'\n",
    "    \n",
    "    #fuzzy match\n",
    "    best_match, score = process.extractOne(str(value), valid_countries)\n",
    "    if score >=80:\n",
    "        return best_match\n",
    "    \n",
    "    #Geopy fallback\n",
    "    try:\n",
    "        location = geolocator.geocode(value, language='en', addressdetails=True, timeout=10)\n",
    "        if location and 'country' in location.raw['address']:\n",
    "            return location.raw['address']['country']\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return 'unknown'\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4978fafb",
   "metadata": {},
   "source": [
    "14. Apply Cleaning Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5372bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply function to the dataframe\n",
    "\n",
    "#step 1: get unique values\n",
    "unique_values = df['country'].unique()\n",
    "\n",
    "#step 2: Build Mapping only once\n",
    "mapping = {}\n",
    "\n",
    "for val in unique_values:\n",
    "    cleaned = clean_country(val)\n",
    "    mapping[val] = cleaned\n",
    "    time.sleep(1) # respect geopy's rate limit (1 request per second)\n",
    "\n",
    "\n",
    "\n",
    "#step3: apply mapping (very fast method)\n",
    "df['clean_country'] = df['country'].map(mapping) # we create a new column\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6f1f0b",
   "metadata": {},
   "source": [
    "Check the number of ubique values in cleaned_country column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11eb656",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_country'].nunique() #shows the number of unique country values after cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37ddcc4",
   "metadata": {},
   "source": [
    "Review and validate new country column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db086e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Set max rows to display (use None to show all)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# Now this will show all rows\n",
    "print(df['clean_country'].value_counts())\n",
    "\n",
    "# Optional: reset back to default after\n",
    "pd.reset_option('display.max_rows')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1bbff6",
   "metadata": {},
   "source": [
    "15. Manually correct specific country issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df7902d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after inspection, we observed that some states in the united states of america from dirty 'country' column were recorded as india in the clean_country column\n",
    "# update such columns to their right values\n",
    "df.loc[df['country'].isin(['Virginia', 'Virgin Islands (US)']), 'clean_country'] = 'United States'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafe1760",
   "metadata": {},
   "outputs": [],
   "source": [
    "#update nigerian values\n",
    "df.loc[df['country'].isin(['Nigerian b','Nigeria see','Nigen','NigeriaCa',' I\"m Nigeria', 'Nigeria to','Nigeriaxd','NigeriaRSTS380','Nigeria+234',\n",
    "                           'Nj','Nijjggggg','New Nigeria', 'Nigeria Kano State', 'United Statesnigerian','Nigerians',\n",
    "                           'Nigerani','ONDO state','Nigeria Aba state','Nigey','nig', 'Nigeria mnoko?!9', 'Nigeria Nigeria',\n",
    "                           'Nigeria you','Nig']), 'clean_country'] ='Nigeria'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872acd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#update values to unknown\n",
    "df.loc[df['country'].isin(['Mouse','unknown']), 'clean_country'] = 'unknown'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0e50db",
   "metadata": {},
   "source": [
    "16. Drop Unnecessary Columns to make the dataset smaller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff77b2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the columns that are not required\n",
    "df.drop(columns=['business_id','longitude','latitude','approval_status','meta','id'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347a653a",
   "metadata": {},
   "source": [
    "17. Convert Boolean columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf133e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bool_columns = ['service_provider_down', 'check_background','duplicate_check','selfie_to_human_review','address_exists', 'direct_feedback']\n",
    "df[bool_columns] = df[bool_columns].astype(bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c4f08e",
   "metadata": {},
   "source": [
    "18. Check for Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf8f393",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum() # no duplicates recorded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7b1767",
   "metadata": {},
   "source": [
    "19.Replace Dirty Country Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a542ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['country']) # drop column country as it contains the list of dirty country list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d36dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={'clean_country':'country'}) #rename clean_country column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce801fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns # check the column list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487e1e36",
   "metadata": {},
   "source": [
    "20. Save Cleaned Data and Export to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9890d68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('verification_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275b364b",
   "metadata": {},
   "source": [
    "21. Connect to MySql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b015b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql\n",
    "\n",
    "try:\n",
    "    connection = pymysql.connect(\n",
    "        host='localhost',\n",
    "        port=3306,\n",
    "        user='root',\n",
    "        password='password',\n",
    "        database='verifications_project',\n",
    "        connect_timeout=10\n",
    "    )\n",
    "    print('Connected to mySQL server successfully')\n",
    "\n",
    "except pymysql.MySQLError as e:\n",
    "    print('Error connecting to MySql:', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0fe5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#format of connection string is\n",
    "# mysql+pymysql;//username:password@host:port/database_name\n",
    "\n",
    "import sqlalchemy\n",
    "\n",
    "engine= sqlalchemy.create_engine('mysql+pymysql://root:password@localhost:3306/verifications_project', execution_options = {\"fast_executemany\":True})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b8c74b",
   "metadata": {},
   "source": [
    "22. Insert in Chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e907a8d",
   "metadata": {},
   "source": [
    "<p>When importing a large DataFrame into a MySQL Server, you want to optimize both the efficiency and speed of the import process. We will attempt to chunk the data instead of trying to insert all 3 million rows at once, we can break the DataFrame into smaller chunks and insert them sequentially. This reduces memory overhead and can improve performance.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249b9fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "\n",
    "#define chunk size\n",
    "chunk_size = 10000\n",
    "\n",
    "#using a try and except block, iterate over chunks and insert data\n",
    "\n",
    "\n",
    "for i in range(0, len(df), chunk_size):\n",
    "    chunk = df.iloc[i: i+ chunk_size]\n",
    "\n",
    "    try:\n",
    "        with engine.begin() as connection:  # create a new DB connection with auto-rollback on failure\n",
    "            chunk.to_sql(\n",
    "                name='verifications', #database table name\n",
    "                con=engine,\n",
    "                if_exists='append',\n",
    "                index=False,\n",
    "                method='multi',\n",
    "                chunksize=chunk_size\n",
    "            )\n",
    "        print(f'chunk {i} inserted successfully')\n",
    "\n",
    "    except SQLAlchemyError as e:\n",
    "        print(f\" Error inserting chunk starting at row {i}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23571a0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
